{"posts":[{"title":"A multi-thread media stream in C++ based on OpenCV and OpenGL","text":"Video DisplayWe did the video capture using OpenCV, and the video display . FPS Modification Built-in function for FPS modificationIn OpenCV, there is a built-in function set() aimed to change the frame rate. However, when you set a lower fps, OpenCV will not add or remove frames from the video stream. Instead, it may internally drop frames as needed to achieve the desired fps. The mechanism depends on the backend and video source. 1234567bool glcvCanvas::setVideo(const std::string&amp; videoPath) { cv::VideoCapture cap(videoPath); if (!cap.isOpened()) return false; LOGI(&quot;Number of frames in the video file: %d\\tVideo FPS: %f\\n&quot;,cap.get(CAP_PROP_FRAME_COUNT), cap.get(cv::CAP_PROP_FPS)); LOGI(&quot;Video Capture Backend: %s\\n&quot;, cap.getBackendName()); //cap.set(cv::CAP_PROP_FPS, 5); Re-encode the videoIn our case, the backend for video capture is FFmpeg. Although the FFmpeg supports frame rate modification by inserting command like 1ffmpeg -i &lt;input&gt; -filter:v fps=30 &lt;output&gt; it will also drop or duplicate frames as necessary. That is an undesired side effect in our case. Therefore, in order to achieve the target frame rate without dropping frames, a dynamic delay is introduced for each frame. The multi-threading section will be discussed in the next part.The LOGI() macro function is defined as the wxLogInfo() (exactly the same as wxLogVerbose()) from wxWidgets libarary. 12345678910111213141516171819202122232425262728double target_fps = cap.get(cv::CAP_PROP_FPS);double frame_time = 1000.0 / target_fps;LOGI(&quot;Number of frames: %f\\tVideo FPS: %f\\n&quot;, cap.get(cv::CAP_PROP_FRAME_COUNT), target_fps);{ std::unique_lock&lt;std::mutex&gt; medialock(mp.mediaMutex); mp.thread_running = true;}cv::Mat frame;while (!stopThread) { auto start = std::chrono::high_resolution_clock::now(); cap &gt;&gt; frame; if (frame.empty()) break; set(frame); auto end = std::chrono::high_resolution_clock::now(); auto elapsedMilliseconds = std::chrono::duration_cast&lt;std::chrono::milliseconds&gt;(end - start).count(); if (elapsedMilliseconds &lt; frame_time) { std::this_thread::sleep_for(std::chrono::milliseconds(static_cast&lt;long long&gt;(frame_time - elapsedMilliseconds))); }}cap.release();{ std::unique_lock&lt;std::mutex&gt; medialock(mp.mediaMutex); mp.thread_running = false;}c_v.notify_one(); Muti-thread media sources selectionIn most cases, if there’s a thread currently running which is manipulating the media, we expect it would be safely terminated before proceeding with new operations. Therefore, this section will check for a running thread, request the video to stop, wait for the thread to stop, reset the video playing flag, and unlock the mutex. Detachable or joinable:Detachable threads are typically used when the parent thread does not need to wait for or synchronize with the detached thread’s completion.Joining a thread involves waiting for the thread to finish executing and obtaining its return value or status. This can be useful when the parent thread relies on the results of the child thread or needs to ensure proper synchronization before proceeding.In our case, we expect the previous video process to be terminated properly as we selecting a new source to display. Note that the logging information for the running thread is probably blocked on the log view, but this part of multi-thread code does it functionality properly. 1234567891011121314151617181920212223242526272829void targetCanvas::selectSource(int src){ image_source = (image_source_t)(src % TARGET_LAST); if (image_source == TARGET_FILE) { std::unique_lock&lt;std::mutex&gt; medialock(mediaMutex); if (thread_running) { stopThread = true; c_v.wait(medialock, []{return !thread_running; }); stopThread = false; } std::string fileExtension = getFileExtension(imageFile.ToStdString()); std::transform(fileExtension.begin(), fileExtension.end(), fileExtension.begin(), ::tolower); medialock.unlock(); if (fileExtension == &quot;mp4&quot; || fileExtension == &quot;avi&quot; || fileExtension == &quot;mov&quot;) { setVideo(imageFile.ToStdString()); } else { cv::Mat image = cv::imread(imageFile.ToStdString(), cv::IMREAD_ANYCOLOR); if (image.empty()) { // ... handle issues } else { set(image); } } }}","link":"/2023/10/24/A-media-stream-based-on-OpenCV-and-OpenGL/"},{"title":"Ad prediction System","text":"Problem Statement How would you build an ML system to predict the probability of engagement for Ads? Search engine Social media platforms Metrics Offline Log Loss AUC does not penalize for “how far off” predicted score is from the actual label. AUC is insensitive to well-calibrated probabilities. Calibration measures the ratio of average predicted rate and average empirical rate Use cross-entropy loss. Online Overall revenue Revenue is basically computed as the sum of the winning bid value when the predicted event happens. Overall ads engagement rate Counter metrics, tracking this to see if the ads are negatively impacting the platform. Architecture Advertiser flow Query-based targeting User-based targeting Interest-based targeting Set-based targeting User flow Training Data Generation Training data generation through online user engagement Advertiser should specify which action to consider positive or negative (ignored or negative feedback). Balancing positive and negative examples Model Recalibration Negative downsampling make our predicted model output be in the downsampling space. It’s critical to recalibrate our score before sending then to auction. $q = \\frac{p}{p+(1-p)/w}$ $q$ is the re-calibrated prediction score $p$ is the prediction in downsampling space $w$ is the negative downsampling rate Train test split Ad SelectionThe main goal of the ads selection component is to narrow down the set of ads that are relevant for a given query. Phase 1: Quick selection of ads for the given query and user context according to selection criteria Building an in-memory index to store the ads. Phase 2: Rank these selected ads based on a simple and fast algorithm to trim ads. The eventual ranking of ads is based on $(bid)$ $*$ $(prior\\ CPE \\ Score)$ (CPE - cost per engagement). Score boost (a slightly higher score), for a new ad or advertiser (where the system don’t have good prior scores). Time decay Phase 3: Apply the machine learning model on the trimmed ads to select the top ones. System scale: The number of partitions depend of the size of the index The system load (QPS) decides how many times the partition is replicated Ad PredictionAd prediction is responsible for predicting a well-calibrated engagement and quality score of ads.Ads are generally short-lived. So, our predictive model is going to be deployed in a dynamic environment where the ad set is continuously changing over time. Keeping refreshing the model with the latest impressions and engagements after regular intervals. Models Auto non-linear feature generation The easy model is to update it using SGD using mini-batches in logistic regression. It relies on manual effort to create complex feature crosses. Combine additive trees and neural networks to generate features, then feeding into logistic regression. Auction Bid: an advertiser places for that ad. User engagement rate Ad quality score $Ad\\ rank\\ score = Ad\\ predicted\\ score\\ *\\ bid$ $CPE = \\frac{Ad\\ rank\\ of\\ ad\\ below}{Ad\\ rank\\ score}+0.01$ where $CPE$ represents cost per engagement or cost per click $CPC$. A general principle is that the ad will cost the minimal price that still allows it to win the auction. PacingPacing an ad means evenly spending the ad budget over the selected time period. Feature Engineering Ad specific features ad impressions: selecting a cut-off point until which we want to consider the impressions. Advertiser specific features User specific features Context specific features User-ad cross features User-advertiser cross features Example Problem statement: Build a machine learning model to predict if an ad will be clicked. Metrics: Offline： Focusing more on ml metrics instead of revenue metrics or CTR metrics. Normalized Cross-Entropy (NCE): predictive logloss divided by the cross-entropy of the background CTR. Online: Revenue lift: Percentage of revenue changes over a period of time. Model: Calculation&amp;&amp;Estimation Assumptions: $40$k ad requests per second or $100$ billion per month Each observation (record) has hundreds of features, and it takes $500$ bytes to store. Data Size: Historical ad click data includes [user, ads, click_or_not] An estimated 1% CTR (1 billion): $100 * 10^{12} * 500 = 5 * 10^{16} $bytes or $50$ PB. Scale: $100$ million users High-level design Data Lake Batch data prep Batch training jobs Model store Stream data prep pipeline Model Serving Aggregator Service:","link":"/2023/12/25/Ad-prediction-System/"},{"title":"Depthwise Separable Convolutions","text":"Inception ModulesIn a convolutional layer, a single convolution kernel is tasked simultaneously mapping cross-channel correlations and spatial correlations. The inception module is to make this process easier and reduce computational expense by decoupling the depthwise convolution (i.e. spatial convolution over each channel) and pointwise convolution (i.e. 1x1 kernel for cross-channel operations). A typical Inception module includes the following components: 1x1 cross-channel operation for the input feature map. Multisized Convolutional Kernels (1x1, 3x3, 5x5): Convolutional kernels of various sizes are applied simultaneously to the input feature maps to capture spatial features at different scales. Pooling Layer: It helps to capture the context of features and reduce the overfitting. Channel Concatenation: The output of the module contains aggregated information from features at all scales. Depthwise Seperable ConvolutionAs introduced from Xception, a depthwise seperable convolution can be understood as an Inception module with a maximally large number of towers. The DSC provides dedicated processing for each channel, instead of sharing the same convolutional kernels across multiple channels. Xception architecture is a linear stack of depthwise separable convolution layers with residual connections. Differences between the Inception Modules and the Depthwise Separable Convolution The order of the operations: depthwise separable convolutions as usually implemented (e.g. in TensorFlow) perform first channel-wise spatial convolution and then perform 1x1 convolution, whereas Inception performs the 1x1 convolution first. The DSC preserves the independence of spatial features by processing the spatial information of each channel first. The Inception starts with the fusion and reduction of features, followed by the extraction of spatial features. Non-linearity: The presence or absence of a non-linearity after the first operation. In Inception, both operations are followed by a ReLU non-linearity, however depthwise are able convolutions are usually implemented without non-linearities. The ReLU in Inception following each convolution operation is to increase the network’s non-linear capability in order to capture more complex features and patterns. The ReLU in DSC might not be used between the depthwise (spatial) and the pointwise (1x1) convolution. This is because non-linear activation function could potentially disrupt the feature representation between two consecutive convolutional steps. The main advantage of DSC is to reduce computational load and the number of parameters. Code for Xception As demonstrated in the paper, the architecture can be displayed as the following. This model can be accessed directly from Keras. Some arguments in the Keras model need to be explained: include_top: this parameter is commonly used to determine whether to include the fully connected layers at the top of the model. This option is particularly important when using pretrained networks. Adaptability to Specific Tasks: Pretrained networks are typically trained on large datasets (like ImageNet) for recognizing thousands of categories. Feature Extractor: When the top fully connected layers are not included, the model can be used as a feature extractor. Fine-Tuning: In transfer learning, we usually only train the newly added layers while freezing the rest of the pretrained model. pooling: Optional pooling mode for feature extraction when include_top is False. References ⭐ Rethinking the Inception Architecture for Computer Vision Xception: Deep Learning with Depthwise Separable Convolutions","link":"/2023/11/08/Depthwise-Separable-Convolutions/"},{"title":"Intro to DenseNet","text":"Abstract DenseNet is detached from the stereotypical thinking of deepening the number of network layers (ResNet) and widening the network structure (Inception) to improve the performance of the network. The LimitationsDenseNet and other related networks all share a key characteristic: they create short path from early layers to later layers. There exists some limitations from its pioneering works. ResNet combines features through summation (additive identity transformations). Stochastic depth improves the training of deep residual networks by dropping layers randomly during training. This makes the state of ResNets similar to (unrolled) RNN.The major difference between DenseNet and related works is to use concatenating features-maps. Advantages Improve flow of information and gradients through the network Mitigate the gradient vanishing problem Fewer parameters No need to relearn redundant feature-maps DenseNet layer explicitly differentiates between new information and preserved information Each layer is very narrow (e.g., 12 filters per layer) Regularizing effect to reduce overfitting Network Advantage Disadvantage Summation (ResNet) Parameter Efficiency Improved Gradient Flow Identity Shortcut Feature Mixing Capacity Limitation Concatenation (DenseNet) Feature Preservation Enhanced Feature Propagation Flexibility Increased Computational Cost Memory-intensive StructureIn a summary, whereas traditional convolutional networks with $L$ layers have $L$ connections, the densenet network has $\\frac{L(L+1)}{2}$ direct connections.Our basic dense connectivity can be represented as:$$ x_l = H_l([x_0, x_1, …., x_{l-1}])$$ Dense Block: To facilitate down-sampling within the architecture and to address concatenation issues arising from changes in the sizes of feature-maps between two blocks. Composite function: $H_l(.)$ consists of three consecutive operations: batch normalization, ReLU, and a $3\\times3$ convolution. The conv layer shown above corresponds the sequence BN-ReLU-Conv. Transition Layer: it does convolution and pooling. Bottleneck layers: a $1\\times1$ convolution can be introduced as bottleneck layer before each $3\\times3$ convolution to reduce the number of input feature-maps and to improve computational efficiency. DenseNet-B: BN-ReLU-Conv($1\\times1$)-BN-ReLU-Conv($3\\times3$) Parameters: There is a general trend that DenseNets perform better as $L$ and $k$ increase without compression or bottleneck layers. Growth Rate: The growth rate $k$ regulates how much new information each layer contributes to the global state. Compression: $0&lt;\\theta&lt;1$ is referred to as the compression factor. It is set to 0.5 to improve model compactness which is to reduce the number of feature-maps at transition layer. DenseNet-BC: both the bottleneck and transition layers with $\\theta&lt;1$ are used. The following heatmaps shows the average absolute weight to serve as a surrogate for the dependency of a convolution layer on its preceeding layer. The top row of each heatmap indicates that the transition layer outputs many redundant features. The right-most column shows that the weights of the transition layer also spread their weight across all layers within the preceding dense block. The DenseNet-BC helps to alleviate this problem. The column shown on the very right suggests that there may be some more high-level features produced late in the network. Memory-efficient Implementation References ⭐ Densely Connected Convolutional Networks","link":"/2024/01/30/DenseNet/"},{"title":"Entity Linking System","text":"Problem StatementGiven a text and knowledge base, find all the entity mentions in the text(Recognize) and then link them to the corresponding correct entry in the knowledge base(Disambiguate). There are two parts to entity linking: Named-entity recognition Disambiguation Applications: Semantic search Content analysis Question answering systems/chatbots/virtual assistants, like Alexa, Siri, and Google assistant Metrics Offline metricsOffline metrics will be aimed at improving/measuring the performance of the entity linking component. Name Entity Recognition (NER) The tagging scheme: IOB2 (aka BIO) B - Beginning of entity I - inner token of a multi-token entity O - non-entity Performance measurement $Precision = \\frac{no. ;of; correctly ;recognized ;named ;entities}{no. ;of; total ;recognized ;named ;entities}$ $Recall = \\frac{no. ;of; correctly ;recognized ;named ;entities}{no. ;of; named ;entities ;in ;corpus}$ $F1-score = 2\\frac{precisionrecall}{precision+recall} $ Disambiguation Precision Recall does not apply here as each entity is going to be linked (to either an object or Null). Named-entity linking component F1-score as the end-to-end metric True positive: an entity has been correctly recognized and linked. True negative: a non-entity has been correctly recognized as a non-entity or an entity that has no corresponding entity in the knowledge base is not linked. False positive: a non-entity has been wrongly recognized as an entity or an entity has been wrongly linked. False negative: an entity is wrongly recognized as a non-entity, or an entity that has a corresponding entity in the knowledge base is not linked. Micro-averaged metrics Aggregate the contributions of all documents to compute the average metrics. Focus on the overall performance, do not care if the performance is better for a certain set. $Precision = \\frac{\\sum^{n}{i=1}TP_i}{\\sum^{n}{i=1}TP_i + \\sum^{n}_{i=1}FP_i}$ $Recall = \\frac{\\sum^{n}{i=1}TP_i}{\\sum^{n}{i=1}TP_i + \\sum^{n}_{i=1}FN_i}$ Micro-averaged F1-score Macro-averaged metrics Compute the metric independently for each document and takes the average (equal weightiage). $Precision = \\frac{\\sum_{i=1}^n P_{di}}{n}{where; n= no. ;of; documents,; P{di}= precision; over; document; i}$ $Recall = \\frac{\\sum_{i=1}^n R_{di}}{n}{where; n= no. ;of; documents,; R{di}= recall; over; document; i}$ Macro-averaged F1-score Online metricsOnline metrics will be aimed at improving/measuring the performance of the larger system by using a certain entity linking model as its component. (A/B experiments) Search engines: i.e. session success rate Virtual Assistants: i.e. user satisfaction Architecture Model generation path Named Entity Recognition Responsible for building models to recognize entities (i.e. person, organizations) Named Entity Disambiguation Candidate generation Reduce the size of the knowledge base to a smaller subset of candidate entities Linking Metrics NER and NED seperately, Entity linking as a whole. Model execution path Modeling NER Modeling Contextualized text representation ELMo Raw word vectors: character-level CNN or Word2Vec Bi-LSTM with forward/backword pass: “Shallow” bi-directional since the two pass LSTMs are trained independently and concatenated naively, therefore, the left and right contexts can not be used ‘simultaneously’. BERT The transformer layer will take advantage of contexts from both directions, but with problems - “sees itself indirectly”. Prediction Objectives: Masked language modeling (MLM) and Next sentence prediction (NSP) Contextual embedding as features Utilize contextual embeddings generated by BERT as features (transfer learning) in our NER classifier. The model will not be modified, only the final layer output is used as embedding features. Fine-tuning embeddings Take the pre-trained models and fine-tune them based on our NER dataset to improve the classifier quality. Disambiguation Modeling Candidate generation Higher recall by building an index where terms are mapped to knowledge base entities. Methods: Based on terms’ names and their concatenations. Make use of referrals and anchor text. Embedding methods + K Nearest Neighbors for a particular term. Linking Higher precision. The prior: for a certain entity mention, how many times the candidate entity under consideration is actually being referred to.","link":"/2024/01/12/Entity-Linking-System/"},{"title":"Feed Based System","text":"Problem StatementDesign a Twitter with 500 million daily active users feed system that will show the most relevant tweets for a user based on their social graph. Metrics Topline is that it’s scientific as well as a business-driven decision. Overall user engagements: comments, likes, and retweets. Business-driven decision: time spent on Twitter. Weighted engagement + Normalized Score (divided by the total number of active users). A higher score equates to higher user engagement. ArchitectureTweet Selection New Tweets + Unseen Tweets: To fetch a mix of newly generated Tweets along with a portion of unseen Tweets from the cache. Edge case: User returning after a while The pool of Tweets keeps on increasing so a limit needs to be imposed on the number of Tweets. Network Tweets + interest/popularity-based Tweets Two-dimensional scheme: selecting network Tweets + potentially engaging Tweets. Benefits Helpful for Bootstrap problems To increase the discoverability on the platform and help grow the user’s network. Ranking Logistic regression Pros and Cons: it is fast to train the major limitation of linear models is that it assumes linearly exists between the input features and prediction. Approaches Train a single classifier for overall engagement. Train seperate models for each engagement action based on production needs (i.e. P(like), P(comments), P(tweet)). MART (multiple additive regression trees) Tree-based models don’t require a large amount of data as they are able to generalize well quickly. Train a single model for the overall engagement. Train specialized predicators to predict different kinds of engagement. Overall engagement + individual predictor of each engagement action -&gt; to build one common preictor P(engagement) and share its output as input into all of your predictors. Deep Learning Seperate neural networks Multi-task neural networks Stacking models and online learning Training tree-based models and neural networks to generate features that we will utilize in a linear model (logical regression) Trees generate features by using the triggering of leaf nodes (result in a boolean feature). Plug-in the output of the last hidden layer as features into the logistic regression models. Logistic regression: this helps the model to re-learn the weight of all tree leaves as well. Main advantage: online learning Using real-time online learning with logistic regression so that we can utilize sparse features to learn the interactions. Training Data Generation Training data generation through online user engagement Balancing positive and negative samples Randomly downsample Train test split We are building models with the intent to forecast the future. Feature EngineeringThe machine learning model is required to predict user engagement on user A’s Twitter feed. Dense features User-author features User-author historical interactions author_liked_posts_3months author_liked_posts_count_1year User-author similarity common_followees topic_similarity tweet_content_embedding_similarity social_embedding_similarity Author features Author’s degree of influence is_verified author_social_rank author_num_followers follower_to_following_ratio Historical trend of interactions on the author’s Tweets author_engagement_rate_3months author_topic_engagement_rate_3months User-Tweet features topic_similarity embedding_similarity Tweet features Features based on Tweet’s content Tweet_length Tweet_recency is_image_video is_URL Features based on Tweet’s interaction num_total_interactions caveat: We need to apply a simple time decay model to weight the latest interaction more than the ones that happened some time ago. Seperate features for different engagements Context-based features day_of_week time_of_day current_user_location lastest_k_tag_interactions approaching_holiday Sparse features unigrams/bigrams of a Tweet user_id tweets_id Diversity Introducing the repetition penalty as adding negative weights To bring the Tweet with repetition three steps in the sorted list.","link":"/2023/12/24/Feed-Based-System/"},{"title":"Hexo DIY","text":"Adding customized features Pin/Unpin a postPin or unpin a post in any order by modifying the source code: Open the “node_modules\\hexo-generator-index\\lib”, then modify the sorting function in “generator.js”; Pin or unpin a post by setting the “top” variable. 123456789101112131415161718module.exports = function(locals) { const config = this.config; const posts = locals.posts.sort(config.index_generator.order_by); // posts.data.sort((a, b) =&gt; (b.sticky || 0) - (a.sticky || 0)); posts.data = posts.data.sort(function(a, b) { if (a.top &amp;&amp; b.top) { if (a.top == b.top) return b.date - a.date; else return b.top - a.top; } else if(a.top &amp;&amp; !b.top) { return -1; } else if(!a.top &amp;&amp; b.top) { return 1; } else return b.date - a.date; }) Edit the layoutPlease Edit the style files (base.styl and responsive.styl) under the path “node_modules\\hexo-theme-icarus\\include\\style”. Check the issueIf you want to make the home page as 3-column and post pages as 2-column layout, please configure a new ‘_config.post.yml’ file and modify the widescreen layout accordingly. Adjust 2-column / 3-column layout and width Rendering mathematical formulasSome symbols in the formula are being escaped by Hexo’s Markdown processor, so the formula received by MathJax is incorrect. To solve this problem, you can use the code block to wrap the problematic formula. Check this issue Add Night theme Hexo 主题 icarus 4 配置夜间模式 Night Mode hexo-theme-icarus 3.0.0 添加夜间模式 - sbx0’s blog Icons for Markdown Font Awesome 巧用 Font Awesome 装点 Markdown 文档","link":"/2023/09/17/Hexo-DIY/"},{"title":"Image preprocessing in Xception model","text":"The image preprocessing process for the Xception model typically includes the following steps: Size Adjustment: The Xception model expects the input image size to usually be 299x299 pixels. Color Channel Processing: The Xception model expects the input to be a color image, i.e., having 3 color channels (Red, Green, Blue). If your image is grayscale (single-channel), you need to convert it into a three-channel format. Pixel Value Normalization: The preprocessing method used by Xception involves normalizing pixel values to the range of [-1, 1]. This is achieved by dividing the pixel values by 127.5 and then subtracting 1. When normalizing, it’s important to ascertain the exact bit depth of the image and normalize accordingly. This is especially relevant for medical imaging or certain special image formats (like TIFF or DICOM) where the image data might not be standard 8-bit per channel. 12345678910111213141516171819202122def load_image(image_path, size=(299, 299), to_grayscale = False): image = Image.open(image_path) image = image.resize(size) image_array = np.array(image, dtype=np.float32) max_value = np.max(image_array) if max_value &gt; 255.0: image_array /= 65535.0 else: image_array /= 255.0 if len(image_array.shape) == 2 or (len(image_array.shape) == 3 and image_array.shape[2] == 1): # Add an additional dimension if the image is grayscale image_array = np.expand_dims(image_array, axis=-1) if image_array.shape[-1] == 1: # Convert grayscale to RGB image_tensor = tf.convert_to_tensor(image_array, dtype=tf.float32) image_array = tf.image.grayscale_to_rgb(image_tensor) # Subtracting the average color of the image mean_color = np.mean(image_array, axis=(0, 1), keepdims=True) adjusted_image = image_array - mean_color According to the official description of preprocessing in xception model, the normalization step is slightly different in tensorflow. caffe: will convert the images from RGB to BGR, then will zero-center each color channel with respect to the ImageNet dataset, without scaling. tf: will scale pixels between -1 and 1, sample-wise. torch: will scale pixels between 0 and 1 and then will normalize each channel with respect to the ImageNet dataset.The following code is modified from the built-in normalization function from Keras tf.keras.applications.xception.preprocess_input 1234567image_array = np.array(image, dtype=np.float32)max_value = np.max(image_array)if max_value &gt; 255.0: image_array /= 32767.5else: image_array /= 127.5image_array -= 1. Note that OpenCV reads in the image by BGR(Blue, Green, Red) order in default, which is different from PIL (Python Imaging Library) (i.e. RGB (Red, Green, Blue)). 12345# OpenCVimg = cv2.imread(image_path)resized_image = cv2.resize(img, (299, 299))resized_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2RGB) # 转换为 RGBresized_image = resized_image.astype('float32') / 127.5 - 1 If we convert the above rgb image into grayscale format. This conversion is achieved by copying the content of the single grayscale channel into the three RGB channels. 1234def rgb2grayscale(img): img_gray = tf.image.rgb_to_grayscale(img) img_gray_three_channel = tf.image.grayscale_to_rgb(img_gray) return img_gray_three_channel When choosing to draw a single-channel grayscale image or one channel of a three-channel grayscale image, pseudocolor mapping is used to display the grayscale image. This is a common processing method that helps visually distinguish different levels of grayscale. Pseudocolor mapping is mainly used in the following situations: Improving Visualization Data Analysis Data Augmentation: we don’t use this kind augmentation in general. This is because models typically need to learn from the original data, and pseudocolor mapping changes the original pixel values of the image, which may introduce unnecessary complexity or interfere with the learning of model features. References ⭐ Utilities for ImageNet data preprocessing &amp; prediction decoding. Kaggle - ODIR5K: Models for predicting age and sex","link":"/2023/12/12/Image-processing-in-Xception-model/"},{"title":"Intro to AE and VAE","text":"Discrimitive | Generative | Latent Models Discrimitive: it is to learn a mapping between the data and the classes (i.e. learning from historical data and then predicting the probability of A being A or B by extracting its features.). Generative: to learn a probability distribution over the data points without external labels (i.e. learning two differnent models of A and B, then, features are extracted from one object (A or B), and these features are fed into the two models to see the probability. The one with the higher probability determines the result.) Latent: it can be understood as dimension reduction. Generation: refers to the process of computing the data point $x$ from the latent variable $z$. Inference: is the process of finding the latent variable $z$ from the data point $x$ and is formulated by the posterior distribution AutoEncodersThe goal of the standard autoencoder is to reconstruct the input data. In general, it consists of an encoder for compression and a decoder for reconstruction. Vanilla autoencoders are trained using a reconstruction loss (i.e. the L2 distance between the input and output).However, the autoencoder is not good at generalizing unseen dataset. Variational AutoEncodersIn simple terms, a variational autoencoder is a probabilistic version of autoencoders. The loss function of VAE consists of two parts: MSE loss to minimize the difference between the output and the input. In order to avoid overfitting and make the decoder be able to generate unseen data, constraints must be imposed on μ and σ to make the normal distribution they form as close as possible to the standard normal distribution. (KL divergence) Note that the code above is obtained through sampling from a normal distribution. A probability distribution is fully characterized by its parameters (mean and var). This kind of stochastic operation cannot be computed to get the gradients. In other words, we cannot backpropagate through a sampling operation. This is exactly the heart of learning to train variational autoencoders. ReparameterizationIn this case, we use the reparameterization trick. The abstract idea is to keep a fixed part stochastic with epsilon and train the mean and the standard deviation. Next, we will backpropagate through the mean μ and the standard deviation σ that are the outputs of the encoder.$$z=μ+σϵ , ϵ∼N(0,1)$$ Again, the epsilon term introduces the stochastic part and is not involved in the training process. References ⭐ Tutorial on Variational Autoencoders 无监督学习：AE到VAE AE（自动编码器）与VAE（变分自动编码器）简单理解 VAE github code 花式解释AutoEncoder与VAE","link":"/2023/11/28/Intro-to-AE-and-VAE/"},{"title":"Intro to GAN","text":"Supervised or unsupervised? Unsupervised task: generative modeling is an unsupervised task where the model is not told what kind of patterns to look for in the data and there is no error metric to improve the model. Supervised classifier/loss func: the training process of the GAN is posed as a supervised learning problem with the help of a discriminator. Generator and DiscriminatorIn this blog, we will only talk about the vanilla GANs. The core idea that rules GANs is based on the indirect training through D that is also getting updated dynamically. The generator G attempts to produce fake examples that are close to the real distribution so as to fool discriminator D. The discriminator D tries to decide the origin of the distribution. Indirect means that we do not minimize the pixel-wise euclidean distance. The gradients are simply computed from the binary classification of the discriminator. The adversarial loss enables the model to learn in an unsupervised manner. Adversarial Loss$D$ and $G$ play the following two-player minimax game with value function $ V (G, D) $.The loss function consists of two parts, the first part is to find $G$ that minimizes $V$ for a given $D$, then, the second part is to find $D$ that maximizes $V$ for a given $G$. $$ \\min_G\\max_DV(D, G) = \\mathbb{E}_{x{\\sim}p_{data}(x)}[logD(x)] + \\mathbb{E}_{z{\\sim}p_{z}(z)}[log(1 - D(G(z)))] $$ In other words, the adversarial loss can be seperated as the following: $$ L_D = \\frac{1}{M}\\sum_{i=1}^{M}[logD(x^i) + log(1 - D(G(z^i)))] $$$$ L_G = \\frac{1}{M}\\sum_{i=1}^{M}[logD(G(z^i))] $$ Minimize G: For the above loss $L_G$, the $logD(X)$ part is a constant. The input for the discriminator is the generated fake data $G(Z)$. In order to fool the discriminator, the expectation of $D(G(Z))$ is expected to be as larger as possible. Therefore, the loss $log(1 - logD(G(Z)))$ is expected to be minimized. Maximize D: 1st part: The expectation $log(D(X))$ represents that the discriminator input is real data, which is expected to be maximized. 2nd part: The $log(D(G(Z)))$ represents that the discriminator input is the generated fake data, which is supposed to be as small as possible, so that the $log(1 - D(G(Z)))$ is expected to be maximized. Note that, $D$ needs to access both real and fake data while $G$ has no access to the real images. TrainingThe framework corresponding to a minmax two-player game should be trained seperately. The training process can be elaborated as the following image. From (a) to (b), the discriminator is being trained during this process; In (C), the generator is being trained to generate more realistic images to fool the discriminator. In (d), the distribution of real images and generated data are identical, so that the discriminator’s predicted probability is constantly 0.5. References ⭐ Generative Adversarial Networks 对抗生成网络GAN系列——GAN原理及手写数字生成小案例 什么是GAN DCGAN","link":"/2023/12/01/Intro-to-GAN/"},{"title":"K-Fold Cross Validation","text":"The K-fold cross validation is to divide the training data into K parts, using K-1 of them for training and the remaining part for testing. Finally, take the average of the testing errors as the generalization error. This allows for better utilization of the training data.However, I encountered some problems when I was trying two kinds of k-fold cross validation methods. Firstly, we need to understand the significance of data division. The training set is used to train the model and obtain its parameters. The validation set is used for tuning the model’s hyperparameters. The test set is used to evaluate the model’s performance. Plan 1 - Not setting aside a test set in advance Split the dataset into k parts randomly Using K-1 of them for training, the remaining part for testing After K rounds of training, we obtain K different models Plan 2 - Set aside a test set in advance Set aside a test set in advance Split the remaining dataset into k parts randomly Using K-1 of them for training, the remaining part for validation After K rounds of training, we obtain K different models Select the best hyperparameters from these models Use the model with the best hyperparameters and retrain it using all K parts of the data as the training set to obtain the final model In Plan 1: If you adjust hyperparameters using the test set, the goal is to find hyperparameters that slightly improve the performance of all the K-1 models. The aim is to find a relatively good set of hyperparameters. In Plan 2: Hyperparameters are adjusted using the validation set, and the seemingly best model out of the five is chosen. Then, the model is retrained using all data outside the training set, and finally, the test set is used to evaluate its effectiveness. This is to select a final usable model. In this case, the intuition is that the plan 1 helps you evaluate your generalized model but not for choosing a set of model parameters. If your goal is to get a final model with suitable parameters, the plan 2 would be your better choice. References ⭐ 防止K折交叉踩坑 K-折交叉验证(记一个坑)","link":"/2023/11/21/K-Fold-Cross-Validation/"},{"title":"LangChain Callback Deep Dive - Why Standard Tools Trigger Nested Run Events","text":"I’ve recently been developing a multi-agent chatbot featuring a reasoning agent and a conversational agent, each equipped with one or more tools. Notably, the reasoning agent utilizes a ReAct prompt, which can be readily constructed using LangChain. One of the involved tools is a retriever, and the retrieval process can become time-consuming with a large database and a significant number of results to fetch. To address this latency, I’ve decided to stream the reasoning process to the user interface. LangChain conveniently supports this through its CallbackHandler functionality.The following is the initial streamed reasoning content: 1234567891011121314151617181920212223242526272829303132333435['conversation'] Chain starts: Could you provide more details?['conversation'] Chain starts:['conversation'] The LLM is starting ...['conversation'] The LLM ends...Action: ask_userAction Input: Could you please specify what details you are looking for? Are you referring to a specific topic or context?['conversation'] Tool starts...['conversation'] Tool starts...Could you please specify what details you are looking for? Are you referring to a specific topic or context?['conversation'] Tool ends...['conversation'] Tool ends...['conversation'] Chain starts:['conversation'] The LLM is starting ...['conversation'] The LLM ends...I need more information to provide a detailed answer. Could you please specify the topic or context you are referring to?I need more information to provide a detailed answer. Could you please specify the topic or context you are referring to? The repeated ‘Tool starts/ends’ messages were initially quite confusing. I suspected a tool-calling error might be causing the LLM to retry, but this behavior persisted. Interestingly, this issue didn’t occur with the ReAct agent. As this log clearly shows, LangChain’s tracing and callback system treats Tool execution as a two-level process: The Tool Run represents the call to the Tool object (like “ask_user”). The Function Run, a child of the Tool Run, represents the execution of the underlying Python function (like “interact_with_user”). Consequently, I decided to use LangSmith to gain better observability into the agent’s workflow. Let’s go through this process step by step. [‘conversation’] Chain starts: Could you provide more details? LangSmith Trace: This corresponds to the very beginning of the overall LangGraph execution receiving the initial input. Your SSECallbackHandler’s on_chain_start likely logs the input for the root run. [‘conversation’] Chain starts: LangSmith Trace: This maps to the start of the AgentExecutor run (duration 2.19s) inside the user_interactor node. The AgentExecutor itself is a type of Chain, and since it was invoked with the conversation tag (via the node’s function), your handler logs its start. [‘conversation’] The LLM is starting … LangSmith Trace: This maps to the start of the first ChatOpenAI call (duration 0.81s). This call is nested within the first LLMChain (duration 0.82s), which is inside the AgentExecutor. This is the agent figuring out what action to take. [‘conversation’] The LLM ends… LangSmith Trace: Maps to the end of that first ChatOpenAI call. [‘conversation’] Action/Action Input: LangSmith Trace: These lines are the result/output generated by the first ChatOpenAI / LLMChain. The Action/Action Input is possibly the output because of the ‘verbose=True’ setted in the AgentExecutor. [‘conversation’] Chain starts: (The 3rd) LangSmith Trace: This maps to the start of the second LLMChain run (duration 0.81s) within the AgentExecutor. This chain likely takes the observation returned by the ask_user tool and decides on the final response for this agent step. [‘conversation’] The LLM is starting … (The 2nd) LangSmith Trace: Maps to the start of the second ChatOpenAI call (duration 0.81s), which is nested inside the second LLMChain. This is the LLM call that generates the final answer text based on the tool’s result. This is the step highlighted in your LangSmith screenshot. [‘conversation’] The LLM ends… (The 2nd) LangSmith Trace: Maps to the end of the second ChatOpenAI call. The LangChain source code demonstrates this process, _perform_agent_action-&gt;tool.run-&gt;callback_manager.on_tool_start: First on_tool_start (Tool Run): Trigger: Explicitly called by callback_manager.on_tool_start(…) directly within the BaseTool.run method. Represents: The invocation of the Tool object itself (e.g., name=”ask_user”). Hierarchy: Parent run (child of the agent run). Second on_tool_start (Function Run): Trigger: Implicitly triggered by the Runnable execution framework when context.run(self._run, …) is called, using the child callback manager obtained via run_manager.get_child(). Represents: The execution of the underlying Python function logic within self._run (e.g., identified as name=”interact_with_user”). Hierarchy: Child of the first Tool Run. The involved tools are like the following: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556forti_retriever = FortiRetriever()tool_search = create_retriever_tool( retriever=forti_retriever, name=&quot;search_fortinet_products&quot;, description=&quot;Searches and returns excerpts from Fortinet technical documents, Knowledge Base articles.&quot;,)react_tools = [ tool_search, Tool( name=&quot;get_product_version&quot;, func=get_product_version, description=&quot;Get available versions for a specific Fortinet product.&quot; )]@tooldef interact_with_user(input_text: str) -&gt; str: &quot;&quot;&quot;Use this tool when you need to ask the user a clarifying question or request more information. Args: message: The question or message to send to the user channel_id: The channel ID for the current conversation Returns: The user's response as a string &quot;&quot;&quot;&quot;messages&quot; global user_response_queues match = re.match(r&quot;channel_id=(.*?)\\|\\|\\|message=(.*)&quot;, input_text) if not match: return &quot;[Error: invalid tool input format. Expecting 'channel_id=xxx|||message=xxx']&quot; channel_id, message = match.group(1), match.group(2) if not channel_id: return &quot;[Error: No channel ID provided for user interaction]&quot; # Create a queue for this interaction if it doesn't exist if channel_id not in user_response_queues: user_response_queues[channel_id] = Queue() from run import app with app.app_context(): sse.publish({ &quot;type&quot;: &quot;clarification_request&quot;, &quot;gpt_response&quot;: message }, type='message', channel=channel_id) return &quot;[Chain ended: asked user for clarification. Please provide answer in the next request.]&quot;tools = [ Tool( name=&quot;ask_user&quot;, func=interact_with_user, description=&quot;Ask the user clarifying questions when you need more information&quot; )] It looks like for Tools that just wrap a normal Python function, the framework tracks two layers: the tool call itself, plus the function’s execution.But for Tools made using helpers (like create_retriever_tool) that wrap special LangChain objects (like a BaseRetriever), it might only track one layer: just the main tool call. The internal stuff (like the retriever getting documents) is treated as part of that single tool run, instead of getting its own separate child run triggering on_tool_start. References ⭐ Building effective agents langchain/agents langchain_core/tools langchain_core/callbacks","link":"/2025/05/02/LangChain-Callback-Deep-Dive/"},{"title":"Machine Learning Primer","text":"Problem FramingML systems can be divided into two broad categories: predictive ML and generative AI. At a high level, ML problem framing consists of two distinct steps: Determining whether ML is the right approach for solving a problem. The non-ML solution is the benchmark you’ll use to determine whether ML is a good use case Framing the problem in ML terms, and determining which features have predictive power. Prediction Regression models are unaware of product-defined thresholds. If your app’s behavior changes significantly because of small differences in a regression model’s predictions, you should consider implementing a classification model instead. Predict the final decision if possible Hiding the app’s behavior from the model can cause your app to produce the wrong behavior. Understand the problem’s constraints Dynamic thresholds: regression Fixed thresholds: classification Proxy labels substitute for labels that aren’t in the dataset. Note: All will have potential problems. Generation Distillation: To create a smaller version of a larger model Fine-tuning or parameter-efficient tuning Prompt engineering Feature EngineeringOne hot encoding One hot encoding is used for categorical features that have medium cardinality. Problems: Expansive computation and high memory consumption are major problems. One hot encoding is not suitable for Natural Language Processing tasks. Feature hashingCross featureCrossed features / conjunction, between two categorical variables of cardinality c1 and c2. Crossed feature is usually used with a hashing trick to reduce high dimensions. EmbeddingThe purpose of embedding is to capture semantic meaning of features. $d = \\sqrt[4]{D}$ where $D$ is the number of categories. Embedding features are usually pre-computed and stored in key/value storage to reduce inference latency. Numeric features Normalization Standardization Training Pipeline Data partitioning Parquet and ORC files are usually get partitioned by time for efficiency to speed up the query time. Handle imbalance class distribution Use class weights in loss function Use naive resampling Use synthetic resampling Synthetic Minority Oversampling Technique (SMOTE) Randomly picking a point from the minority class Computing the k-nearest neighbors for that point The synthetic points are added between the chosen point and its neighbors. Choose the right loss function Retraining requirements InferenceInference is the purpose of using a trained machine learning model to make a prediction. Imbalanced Workload Upstream process Aggregator Service Worker pool to pick workers; Aggregator Service can pick workers through following ways: Work load Round Robin Request parameter Serving logics and multiple models for business-driven system. Non-stationary problem Update or retrain models to achive sustained performance. One common algo is Bayesian Logistic Regression. Exploration vs. exploitation One common technique is Thompson Sampling where at a time, t, we need to decide which action to take based on the reward. Metrics Offline metrics Metrics like MAE, or R2 to measure the goodness of of fline fit. Online metrics Expose the model to a specific percentage of real traffic. Allocate traffic to different models in production. A/B testing (examples) Amazon SageMaker Linkedin A/B testing","link":"/2024/01/18/Machine-Learning-Primer/"},{"title":"Recommendation System","text":"Problem StatementDisplay media recommendations for a specific user. Type of user feedback includes explicit and implicit feedbacks. The implicit feedback allows collecting a large amount of training data. Metrics Online metrics Engagement rate Videos watched Session watch time (better than the above intuitively) Offline metrics mAP (Mean Average Precision) @ N (length of the recommendation list) $P = \\frac{num_of_relevant_recommendations}{total_num_of_recommendations}$ $P @k = $proportion of all examples above that rank which are from the positive class. Precision represented at each cut off k. $AP@N = \\frac{1}{m}\\sum_{k=1}^{N}P(k)*rel(k)$ where the $P(k)$ only contributes to $AP$ (average precision) if the recommendation at position $k$ is relevant. mAR (Mean Average Recall) @ N mAR @ N at a high-level, showes how well the recommender recalls all the items with positive feedback. $r = \\frac{num_of_relevant_recommendations}{num_of_all_possible_relevant_items}$ F1 Score $F1 score = 2 * \\frac{mAR*mAP}{mAP+mAR}$ For optimizing ratings Optimize the recommendation system for getting the ratings (explicit feedback). $RMSE = \\sqrt{\\frac{1}{N}\\sum_{i=1}^N(\\hat{y_i} - y_i)^2}$ ArchitectureThere are two stages, candidate generation, and ranking. The reason for two stages is to make the system scale. Candidate GenerationThis component focuses on higher recall. Collaborative filtering Nearest Neighborhood Collaborate with similar users to generate candidate media for active user. Caveat: This process will be computationally expensive. This sparsity of this matrix poses the cold start problem (New users ans movies). Matrix factorization Latent vectors (User profile matrix - $nM$, Media profile matrix - $Mm$) can be thought of as features of a movie or a user. It does not require domain knowledge to create profiles, thus, this filtering suffers from the cold start problem. Content-based filtering Make recommendations based on the characteristics or attributes of the media the users have already interacted with. Represent the media as vectors containing the TF of all the attributes; Normalize the TF by the length of the vectors; Multiply normed TF and IDF element-wise to make TF-IDF vectors. Similarity with historical interactions Similarity between media and user profiles The user profiles can be built from historical interactions with media. It requires some initial input from the user regarding their preferences. Embedding-based similarity The NNs allow us to use all the sparse and dense features. Set up a two-tower model with one tower feeding in media only sparse and dense features and the other tower feeding in user-only sparse and dense features. Loss function: $min(abs(dot(u, m) - label))$ The NN technique suffers from the cold start problem. Training Data Generation Generating training samples The data will be split as positive, negative, ot falls into the uncertainty bucket (watch time falls between 20%-80%). Balancing samples by randomly downsampling negative examples. Weighting training examples based on watch time or other features. - One caveat of utilizing weights is that the model might only recommend content with a longer watch time. Train test split: models should be built with the intent to forecast the future. RankerThis component focuses on higher precision. Logistic regression or random forest Reasons Training data is limited. You have limited training and model evaluation capacity. Model explainability is required. An initial baseline is required. Deep NN with sparse and dense features Reasons Hundreds of millions of training examples and capacity should be available. Model interpretability is not required. Important sparse features Videos that the user has previously watched The user’s search terms The sparse features are list-wise (Averaged pooling before being fed to network). Re-ranking: bringing diversity or past watches to the recommendation. Feature Engineering User-based features Context-based features Media-based features Media-user cross features Example: Video RecommendationProblem statement:Build a video recommendation system for YouTube users. Recommendation Youtube is extremely challenging from following major perspectives: Scale: it is required to handle YouTube’s massive user base and corpus; Highly specialized distributed learning algorithms Efficient serving systems Freshness: Balancing new content with well-established videos; Noise: Historical user behavior is inherently difficult to predict because: Sparsity and a variety of unobservable external factors Metadata associated with content is poorly structured Requirements Type Desired requirements Training High throughput with the ability to retrain many times per day Inference Latency from 100ms to 200msBalances between exploration vs. exploitation Metrics:Offline: Precision, recall, ranking loss, and logloss;Online: A/B testing via live experiments to compare Click Through Rates, watch time and Conversion rates. ModelCandidate GenerationNeural network model is used to mimick matrix factorization. A key advantage of that is that arbitrary continuous and categorical features can be easily added to the model. Therefore, our approach can be viewed as a non-linear generalization of factorization techniques. The recommendation problem can be posed as extreme multiclass classification where the prediction problem becomes accurately classifying a specific video watch time $w_{t}$ at time $t$ among millions of videos $i$ (classes) from a corpus $V$ based on a user $U$ and context $C$.$$P(w_t=i|U,C) = \\frac{e^{v_iu}}{\\sum_{j\\in V}{e^{v_ju}}}$$ We use the implicit feedback of the watches to train the model (where a user completing a video is a positive example), where explicit feedback (thumbs up/down, in-product surveys, etc) is extremely sparse; Personally speaking, the diagram might intuitively suggest that video embedding vectors are “outputs” from the softmax layer. However, the pre-learned video embedding vectors are used to calculate their similarity with the user vector through the softmax layer. The final recommendation list is then quickly determined through nearest neighbor search or other efficient methods. Efficient Extreme MulticlassScoring millions of items under a strict serving latency of tens of milliseconds requires an approximate scoring scheme sublinear in the number of classes. Traditional softmax needs speeding up Hierarchical softmax is not able to achieve comparable accuracy Scoring problem reduces to a nearest neighbor search; A/B results were not particularly sensitive to the choice of nearest neighbor search algorithm. EmbeddingEach query is tokenized into unigrams and bigrams and each token is embedded and normalized. High dimensional embeddings for each video in a fixed vocabulary. The network requires fixed-size dense inputs and performs several strategies on concatenated features (embedding). Heterogeneous SignalsMachine learning systems often exhibit an implicit bias towards the past.Video popularity is highly non-stationary but the multinomial. To correct for this, we feed the age of the training example as a feature during training. At serving time, this feature is set to zero (or slightly negative). Label and Context SelectionWe need to take care of the strategy for selecting training examples. Training examples are generated from all Youtube watches to avoid overly exploitation; Generate a fixed number of training examples per user, effectively weighting our users equally in the loss function. In order to prevent a small cohort of highly active users from dominating the loss. Surrogate problemRecommendation often involves solving a surrogate problem and transferring the result to a particular context. Sometimes the choice of surrogate problem is difficult to measure with offline experiments or compute directly. Withhold information from the classifierPrevent the model from exploiting the structure of the site and overfitting the surrogate problem, by discarding sequence information and representing search queries with an unordered bag of tokens. Asymmetric consumption patternsIn order to not leak future information and not ignore any asymmetric consumption patterns. We rollback a user’s history by choosing a random watch and only input actions the user took before the held-out label watch. RankingOur features are segregated with traditional taxonomy of categorical and continuous/ordinal features. Features are further split into “univalent” or “multivalent”; “univalent”: contribute to a single value “multivalent”: contribute to a set of values Features are further split into “impression” or “query”; “impression”: properties of the item, being computed for each item scored; “query”: properties of the user/context, being computed for once per request; Feature EngineeringThe main challenge is in representing a temporal sequence of user actions and how these actions relate to the video impression being scored. The most important signals are those that describe a user’s previous interaction with the item itself and other similar items; Propagate information from candidate generation into ranking models; Introducing “churn” in recommendations (successive requests do not return identical lists), by including features describing the frequency of past video impressions; Embedding Categorical FeaturesVery large cardinality ID spaces: Been truncated by including only the top $N$ after sorting based on their frequency in clicked impressions; Out-of-vocabulary values are simply mapped to the zero embedding; Multivalent categorical feature embeddings are averaged before being fed; Categorical features in the same ID space share underlying embeddings: Sharing embeddings is important for improving generalization, speeding up training and reducing memory requirements. The overwhelming majority of model parameters are in these high-cardinality embedding spaces; Normalizing Continuous FeaturesNeural networks are notoriously sensitive to the scaling and distribution of their inputs. Raw normalized features The continuous feature $x$ with distribution $f$ is transformed into $\\overline{x}$ by scaling the values such that the feature is equally distributed in $[0,1)$, using cumulative distribution (CDF) $\\overline{x} = \\int^{x}_{-\\infty}{df}$. The integral is approximated with linear interpolation. Super- and sub-linear forms ($\\overline{x}^{2}$ and $\\sqrt{\\overline{x}}$) of features Giving the network more expressive power; Input powers $\\overline{x}^{2}$ is found to improve offline accuracy; Modeling Expected Watch TimeWeighted logistic regression; Positive (clicked) impressions are weighted by the ovserved watch time on the video; Negative (unclicked) impressions all receive unit weight;Odds learned by the logistic regression is$$\\frac{\\sum{T_i}}{N - k}$$ $N$ is the number of training examples $k$ is the number of positive impressions $T_i$ is the watch time of the $i$th impressionThe learned odds are approximately $E[T](1 + P)$ $P$ is the click probability, and $E[T]$ is the expected watch time of the impression For inference, the exponential function $e^{x}$ is used as the final activation function to produce these odds that closely estimate expected watch time. Non-negative outputs Adjustable range Non-linear Transformation of linear relationships Approximating probability products When predictions are close to 0, where $e^{x}$ approximates $1+x$, echoing the earlier mentioned approximation relation $E[T](1 + P)$; The intuitive explanation of this formula is: When there is no click, the watch time is likely to be close to the baseline expected watch time $E[T]$; When there is a click, the watch time may increase, and this increase is proportional to the click probability $P$; Calculation&amp;&amp;Estimation Assumptions: Video views are $150$ billion per month $10%$ of videos watched are from recommendations (a total of $15$ billion) Data Size: Each feature row takes $500$ bytes to store Each month, we need $800$ billion rows To save costs, we can keep the last $6$ months of data in the data lake, and archive old data in cold storage Bandwidth: Generate recommendation requests for $10$ million users per second Each request will generate ranks for $1k-10k$ videos. Scale: $1.3$ billion users High-level design Challenges: Huge data size Imbalance data High availability Solution 1: Use model-as-a-service, each model will run in Docker containers Solution 2: We can use Kubernets to auto-scale the number of pods Database Resampling data: Down-sampling negative samples; Feature pipeline: To provide high throughput, as we require this to retrain models multiple times, using Spark, Elastic MapReduce or Google DataProc; Model Repos: Store all models, using AWS S3 in general; Scale the design: Kube-proxy could be used to enable the Candidate Generation Service to call Ranking Service directly, reducing latency even further; During inference, one common pattern for the inference component is to frequently pull the latest models from Model Repos based on the timestamp, in order to get the latest model near real-time. Follow-up Q A How do we adapt to user behavior changing over time? 1.Read more about Multi-arm bandit 2.Use the Bayesian Logistic Regression Model so we can update prior data 3.Use different loss functions to be less sensitive with CTR References ⭐ Deep Neural Networks for YouTube Recommendations","link":"/2023/12/24/Recommendation-System/"},{"title":"Intro to Grad-CAM - CNN的可视化","text":"The Grad-CAM (Gradient-weighted Class Activation Mapping) is a generalization of CAM and is applicable to a significantly broader range of CNN model families.The intuition is to expect the last convolutional layers to have the best compromise between high-level semantics and detailed spatial information which is lost in fully-connected layers. The neurons in these layers look for semantic class-specific information in the image. $$L_{Grad-CAM}^c = ReLU(\\sum_k\\alpha_k^cA^k)$$ where $$\\alpha_k^c = \\frac{1}{Z}\\sum_i\\sum_j\\frac{\\partial{y_c}}{\\partial{A_{ij}^k}}$$ The $L_{Grad-CAM}^c$ is the class-discrimitive localization map for any class c.And the $\\alpha_k^c$ represents a partial linearization of the deep network downstream from A, and captures the ‘importance’ of feature map k for a target class c. The $\\frac{1}{Z}\\sum_i\\sum_j$ stands for the global average pooling. We apply a ReLU to the linear combination of maps because we are only interested in the features that have a positive influence on the class of interest, i.e. pixels whose intensity should be increased in order to increase y. Negative pixels are likely to belong to other categories in the image. The formula inside the ReLU bracket follows the same principle as the CAM formula, which is to perform a linear summation of the feature maps from the last convolutional layer, weighted accordingly, to obtain the activation map. The code below uses the gradients of retinal target flowing into the final convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. The model used in this approach has a following structure: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657base_model = model.get_layer('xception')# base_model.summary()last_conv_layer = base_model.get_layer('block14_sepconv2_act')last_conv_layer_output = last_conv_layer.outputintermediate_model = Model(inputs=base_model.input, outputs=last_conv_layer_output)x = intermediate_model.outputx = model.get_layer('global_average_pooling2d_12')(x)x = model.get_layer('dropout_12')(x)final_output = model.get_layer('dense_12')(x)grad_model = Model(inputs=intermediate_model.input, outputs=[last_conv_layer_output, final_output])# Get the gradient of the top predicted class for the input image with respect to the activations of the last conv layerwith tf.GradientTape() as tape: # Forward pass last_conv_layer_output, preds = grad_model(img_batch) # print(preds) # the preds should be a single value (the predicated age) model_output = preds[:, 0] # Assuming the model outputs a single value for age prediction# Compute gradientsgrads = tape.gradient(model_output, last_conv_layer_output)# print(grads.shape) # (1, 10, 10, 2048)# This is a vector where each entry is the mean intensity of the gradient over a specific feature map channelpooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))last_conv_layer_output = last_conv_layer_output[0]heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]heatmap = tf.squeeze(heatmap)# For visualization purpose, we will also normalize the heatmap between 0 &amp; 1heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)# Convert the heatmap to RGBheatmap = np.uint8(255 * heatmap)# Apply the heatmap to the original imagejet = mpl.colormaps[&quot;jet&quot;]# Use RGB values of the colormapjet_colors = jet(np.arange(256))[:, :3]jet_heatmap = jet_colors[heatmap]# Create an image with RGB colorized heatmapjet_heatmap = keras.utils.array_to_img(jet_heatmap)jet_heatmap = jet_heatmap.resize((img.shape[1], img.shape[0]))jet_heatmap = keras.utils.img_to_array(jet_heatmap)superimposed_img = jet_heatmap * 0.002 + imgsuperimposed_img = keras.utils.array_to_img(superimposed_img)# Display the image with attention mapplt.imshow(superimposed_img)plt.axis('off')plt.show() References ⭐ Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization paper-code-grad-cam keras-grad_cam.py pytorch_CAM.py 注意力可视化Grad-CAM","link":"/2023/12/18/Intro-to-Grad-CAM-CNN%E7%9A%84%E5%8F%AF%E8%A7%86%E5%8C%96/"},{"title":"People You May Konw","text":"Problem StatementPeople You May Know (PYMK) is a list of users with whom you may want to connect based on things you have in common, such as a mutual friend, school, or workplace. Clarifying Requirements What’s the objective/motivation? May I assume the motivation of the PYMK feature is to help users discover potential connections and grow their network? How to define ‘connections’? May I assume that people are friends only if each is a friend of the other? What should I consider for forming connections? To recommend potential connections, a huge list of factors must be considered, such as educational background, work experiences, existing connections, historical activities. Should I focus on the most important factors? Define the scale How many connections does an average user have? (like 1000) What’s the total number of users on this platform? (like 1 billion) How many of them are daily active users? (like 300 million) Model Dynamics May I assume that the social graph of most users are not dynamic, meaning that their connections don’t change significantly over a short period. Frame the Problem as an ML task Define the ML Objective Maximize the number of formed connections between users Define the input and the output input: a user output: a list of users Choosing ML methologyThere are commonly two approaches for PYMK: Pointwise Learning-To-Rank (LTR) and Edge Prediction. Pointwise LTR Why this approach: The pointwise LTR transforms a ranking problem to a supervised learning problem, where the model takes user and a candidate connection pair as inputs, and outputs a score or probability. In cold-start scenarios where rich interaction data is lacking, the pointwise LTR can quickly model the similarity between user pairs. Why not this approach: This ignores some social context, the inputs are distinct users Edge Prediction The entire social context can be represented as a graph, where each node represents a user, and an edge between two nodes indicates a formed connection between two users. Use the entire graph as input, and predict the probability of an edge existing between user A and other users Data PreparationWe typically use three types of raw data: User Connections Interactions One challenge with this Feature Engineering Model Development AppendixGraph-basedIn general, a graph represents relations(edges) between a collection of entities(nodes). Graphs can store structural data, there are three general types of prediction tasks that can be performed on structured data represented by graphs: Graph-level like chemical compound Edge-level like PYMK on social media Node-level like predicting whether a person is a spammer or not, given a social network graph","link":"/2025/04/23/People-You-May-Know/"},{"title":"Layer Norm | Batch Norm | Instance Norm | Group Norm","text":"LB | BN | IN | GN in NLP Batch Norm: Normalizes each feature across the entire batch. Rarely used in NLP, because it relies on consistent sequence lengths and large batch sizes, and is sensitive to padding. Layer Norm: Normalizes across the feature dimension for each token independently, making it suitable for variable-length sequences Instance Norm: Originally used in computer vision to normalize each channel within each sample. It is not commonly applied in NLP tasks. Group Norm: Splits the feature (embedding) dimension into groups and performs normalization within each group. It’s occasionally used in NLP when LayerNorm is replaced for better generalization under small-batch or resource-constrained settings. 🤖 Scenarios for BatchNorm vs LayerNormIn terms of operations, BN computes one mean/variance per channel across the entire batch and spatial dimensions, while LN computes mean/variance within each individual sample, across the feature dimension (generally the last dimension dim=-1), independently of other samples and tokens. In ML/CV tasks, data often consists of each column representing a feature, and the processed data typically has interpretability, with different units and properties across columns, hence BN is more commonly used in machine learning tasks. In NLP, Transformer-based models generally represent data as [batch, seq_len, embedding_dim]. LayerNorm is preferred because it normalizes each token’s embedding independently, preserving token-wise semantics and being robust to variable-length sequences without relying on batch-level statistics. 1234567891011121314151617181920212223242526272829303132333435363738394041import torchimport torch.nn as nnclass LayerNorm(nn.Module): def __init__(self, emb_dim, eps=1e-5, affine=True): super().__init__() self.eps = eps self.affine = affine # scale and shift # if affine: self.gamma = nn.Parameter(torch.ones(emb_dim)) self.beta = nn.Parameter(torch.zeros(emb_dim)) def forward(self, x): x_mean = x.mean(dim=-1, keepdim=True) x_var = x.var(dim=-1, keepdim=True) x_norm = (x - x_mean) / torch.sqrt(x_var + self.eps) return self.gamma * x_norm + self.betaclass BatchNorm2d(nn.Module): def __init__(self, num_channels, eps=1e-5, momentum=0.1): super().__init__() self.eps = eps self.momentum = momentum self.gamma = nn.Parameter(torch.ones(num_channels)) self.beta = nn.Parameter(torch.zeros(num_channels)) self.register_buffer('running_mean', torch.zeros(num_channels)) self.register_buffer('running_var', torch.ones(num_channels)) def forward(self, x): if self.training: mean = x.mean(dim=(0, 2, 3), keepdim=True) # (C,) var = x.var(dim=(0, 2, 3), unbiased=False, keepdim=True) self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.view(-1) self.running_var = (1 - self.momentum) * self.running_var + self.momentum * var.view(-1) else: mean = self.running_mean.view(1, -1, 1, 1) var = self.running_var.view(1, -1, 1, 1) x_hat = (x - mean) / torch.sqrt(var + self.eps) return self.gamma.view(1, -1, 1, 1) * x_hat + self.beta.view(1, -1, 1, 1) 🧠 Summary Comparison: BatchNorm vs LayerNorm Feature BatchNorm LayerNorm Depend on batch size? ✅ Requires cross-sample statistics ❌ Independent of batch size Handles variable-length sequences? ❌ Padding affects statistics ✅ Each token is normalized independently Inference stability? ❌ Uses moving average, can be unstable ✅ Consistent behavior during inference Multi-GPU / small batch stability? ❌ Very unstable ✅ Not affected at all Suitable for Transformer architecture? ❌ Breaks token-wise independence ✅ Perfectly matches token-wise design LB | BN | IN | GN in CVConsidering feature maps with shape (N, C, H, W): N: batch size C: number of channels H, W: spatial dimensions Different normalization methods apply statistics over different dimensions: Batch Norm (BN): Normalizes each channel using statistics computed across the batch (N) and spatial dimensions (H, W). Each channel shares a single mean and variance across all samples and locations. Layer Norm (LN): Normalizes across all channels and spatial dimensions within each sample, making the entire feature map of a sample zero-mean and unit-variance. Instance Norm (IN): Normalizes each channel independently per sample, using only the spatial dimensions (H, W) of that channel. Group Norm (GN): Divides channels into groups (e.g., 32 groups), then normalizes the features within each group across spatial dimensions, per sample. 🤖 Scenarios for Instance Norm and Group Norm: Instance Norm: it is particularly suited for style transfer tasks and image generations. Instance normalization helps maintain the individuality and unique style of each image, free from batch-level interference. Group Norm: Particularly effective in scenarios with small batch sizes or batch size = 1, such as object detection and image segmentation. By normalizing over groups of channels within each sample, it provides stable training dynamics and better generalization where BatchNorm may fail. 1234567891011121314151617181920212223242526272829303132class InstanceNorm(nn.Module): def __init__(self, num_channels, eps=1e-5, affine=True): super().__init__() self.eps = eps self.affine = affine self.gamma = nn.Parameter(torch.ones(num_channels)) self.beta = nn.Parameter(torch.zeros(num_channels)) def forward(self, x): x_mean = x.mean(dim=(2, 3), keepdim=True) x_var = x.var(dim=(2, 3), keepdim=True, unbiased=False) x_norm = (x - x_mean) / torch.sqrt(x_var + self.eps) return self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1)class GroupNorm(nn.Module): def __init__(self, num_groups, num_channels, eps=1e-5, affine=True): assert num_channels % num_groups == 0 self.num_groups = num_groups self.num_channels = num_channels self.eps = eps self.affine = affine self.gamma = nn.Parameter(torch.ones(num_channels)) self.beta = nn.Parameter(torch.zeros(num_channels)) def forward(self, x): Batch, Channel, Height, Width = x.shape x = x.view(Batch, self.num_groups, -1) x_mean = x.mean(dim=-1, keepdim=True) x_var = x.var(dim=-1, keepdim=True, unbiased=False) x_norm = (x - x_mean) / torch.sqrt(x_var + self.eps) x_norm = x_norm.view(Batch, Channel, Height, Width) return self.gamma.view(1, -1, 1, 1) * x_norm + self.beta.view(1, -1, 1, 1) References ⭐ 归一化方法 BN、LN、IN、GN、SN Normalization之BN, LN, IN, GN","link":"/2023/11/14/Layer-Norm-Batch-Norm-Instance-Norm-Group-Norm/"},{"title":"Search Ranking","text":"Problem StatementAsk for questions: Scale, Scope, Personalization. Scope: general search or specialized search? Scale: number of websites? QPS (queries per second)? Personalization: logged-in user or not MetricsOnline Metrics Click-through rate $$Click through rate = \\frac{Number of clicks}{Number of impressions} $$ Successful session rateInclude the Dwell Time (the length of time a searcher spends viewing a webpage after they’ve clicked a link on a SERP-search engine per page) into consideration. $$Session success rate = \\frac{no. of successful sessions}{no. of total sessions}$$ CaveatAttention is to consider zero-click searches. A SEPR may answer the searcher’s query at the top.Time of success: a low number of queries per session - successful search session. Offline Metrics Normalized discounted cumulative gain (NDCG) $$CG_p = \\sum_{i=1}^{p}rel_i$$ 123In the cumulative gain, the rel = relevance rating of a documenti = position of documentp = the position up to which the documents are ranked $$DCG_p = \\sum_{i=1}^{p} \\frac{rel_i}{log_2(i+1)}$$ 1The discounted cumulative gain allows us to penalize the search engines's ranking if highly relevant documents appear lower in the result list. Caveat: $DCG$ can’t be used to compare the search engine’s performance across different queries on an absolute scale. (the $DCG$ for a query with a longer result list may be higher due to its length instead of its quality.) $$NDCG_p = \\frac{DCG_p}{IDCG_p}$$ where $IDCG$ is ideal discounted cumulative gain. 1NDCG normalizes the DCG in the 0 to 1 score range by dividing the DCG by the max DCG or the IDCG of the query. $$NDCG = \\frac{\\sum_{p=i}^NNDCG_i}{N}$$An $NCDG$ value near 1 indicates good performance by the search engine. Caveat$NCDG$ does not penalize irrelevant search results. Architecture Document SelectionDocument selection is more focused on recall. Selection criteria Inverted index: an index data structure that stores a mapping from content. Go into the index and retrieve all the documents based on the above selection criteria. -&gt; sorted relevant documents according to their relevance score. -&gt; forward the top documents to the ranker. Relevance scoring scheme Weighted linear scheme from: Terms match: inverse document frequency or IDF score Document popularity Query intent match Personalization match. Feature Engineering Query-specific features Query historical engagement Query intent Example: Users search for ‘pizza place’, the intent here is ‘local’. Document-specific features Page rank Document engagement radius Context-specific features Time of search Recent events Searcher-document features Distance Historical engagement Query-document features Text match Unigram or bigram TF-IDF (term frequency-inverse document frequency) match score can also be an important relevance signal for the model. Query-document historical engagement, like click rate Embeddings. To calculate similarity score between the query’s vector and the document’s vector. Training Data Generation Pointwise approach Assign positive/negative samples based on click rate or successful session rate. Caveat: Less negative examples. To remedy it, we use random negative examples. Pairwise approach The loss function looks at the scores of document pairs as an inequality instead of the score of a single document. RankingStage-wise approach is being used. First stage will focus on the recall of the 5-10 documents in the first 500 results while the second stage will ensure precision of the top 5-10 relevant documents. Stage 1: pointwise approach Simple ML algorithms Performance: AUC or ROC Stage 2: pairwise approach LambdaMART LambdaRank Example: Rental Search Ranking Problem statement: Sort results based on the likelihood of booking. Build a supervised ML model to predict booking likelihood -&gt; Binary classification model. Metrics: Offline metrics: conversion rate: $conversion_rate = \\frac{number_of_bookings}{number_of_search_results}$ Online metrics: $DCG$ $NDCG$ $IDCG$ Requirements Training: Imbalance data and clear-cut session Train/validation data: Split data by time to mimic production traffic. Inference: Serving: low latency Under-predicting for new listings: not enough data for model to estimate likelihood for brand new listings. Model Feature Engineering Geolocation (latitude/longitude): raw data is not smooth, we can use log of distance from the center center. Favourite places. Train data: decide the length of training data by running experiments. Model: Input: User data, search query, and Listing data. Output: binary classification, i.e., booking a rental or not fully connected layers as baseline, or more modern architecture (VAE). Calculation&amp;&amp;Estimation Assumptions: $5 * 30 * 10^8 = 1.25 b$ where 5-5 times per year per user, 30-see 30 rentals before booking, and 100m monthly active users. Data Size: $500 * 1.25 * 10^9$ where each row takes 500 bytes to store. Keep the last 6 months or 1 year of data in the data lake, and archive old data in cold storage. Scale: Support $150$ million users. High-level design Feature pipeline: Process online features and store key-value feature pairs. Feature Store: We need low latency ($&lt;10ms$) during inference: MySQL Cluster, Redis, and DynamoDB. To scale and serve millions of requests per second by scaling out Application Servers and use Load Balancers to distribute the load. Scale Search/Ranking services. Model Store is a distributed storage, like S3. Log all candidates as training data which is sent from Search Service to cloud storage or Kafka cluster. Follow up Questions","link":"/2023/12/19/Search%20Ranking/"},{"title":"Self-driving Image Segmentation","text":"Problem StatementDesign a self-driving car system focusing on its perception component (semantic image segmentation in particular) Subtasks Object detection Semantic segmentation Semantic segmentation can be viewed as a pixel-wise classification of an image. Instance segmentation It combines object detection and segmentation to classify the pixels of each instance of an object. Scene understanding Movement plan Metrics Component level metric Goal: Higher pixel-wise accuracy for objects belonging to each class. IoU (Intersection over Union) This will be used as the offline metrix. $IoU = \\frac{|P_{pred}\\cap P_{gt}|}{|P_{pred}\\cup P_{gt}|}$ “area of overlap”: means the number of pixels that belong to the particular class in both the prediction and ground-truth “area of union” refers to the number of pixels that belong to the particular class in the prediction and in ground-truth, but not in both (the overlap is subtracted). The mean IoU is calculated by taking the avaerage of the IoU for each class End-to-end metric Manual intervention Simulation errors Architecture Overall architecture for self-driving vehicle The object detection CNN detects and localizes all the obstacles and entities Drivable region detection CNN: Action predictor RNN is information that allows it to extract a drivable path for the vehicle. Semantic image segmentation (from raw pixel-wise boundaries) System architecture for semantic image segmentation The real-time driving images are captured and manually given pixel-wise labels. Training Data Generation Human-labeled data Open Source dataset Training data enhancement through GANs Generating new training images Ensuring generated images have different conditions (e.g. weather and lighting conditions) Image-to-image translation (cGANs) Targeted data gathering Modeling SOTA segmentation models FCN Segmentation is a dense prediction task of pixel-wise classification. Major characteristics Dynamic input size: the fully connected layers are replaced by convolutional layers at the end of the regular convolution and pooling process. Skip connection: the initial layers capturing good edges are connected with the coarse pixel-wise segmentations. U-Net It is built upon FCN and commonly used for semantic segmentation-based vision applications. Downsampling increases info about what objects are present, decreases info about where objects are present. Upsampling creates high-resolution segmented output by making use of skip connections. Mask R-CNN It is used for instance segmentation. Faster R-CNN for object detection and localization and FCN for pixel-wise instance segmentation of objects. Backbone of a CNN followed by a Feature Pyramid Network (FPN) which extracts feature maps at different scale. The feature maps are fed to the Region Proposal Network (RPN). These proposals are fed to the RoI Align layer that extracts the corresponding ROIs (regions of interest) from the feature maps to align them with the input image properly. The ROI pooled outputs are fixed-size feature maps that are fed to parallel heads of the Mask R-CNN. Mask R-CNN has three parallel heads to perform Classification, Localization, and Segmentation. Transfer learning Retraining topmost layer Update the final pixel-wise prediction layer in the pre-trained FCN This approach makes the most sense when The data is limited. You believe that the current learned layers capture the information that you need for making a prediction. Retraining top few layers Update the upsampling layers and the final pixel-wise layer. This approach makes the most sense when Have a medium-sized dataset Shallow layers generally don’t need training because they are capturing the basic image features, e.g., edges Retraining entire model Laborious and time-consuming The dataset has completely different characteristics from the pre-trained network","link":"/2024/01/17/Self-driving-Image-Segmentation/"},{"title":"VS Property Configuration C&#x2F;C++","text":"For my own projects, the project should be set up as the following: From terminal add environment values: 123setx okFP_SDK C:\\Program Files\\Opal Kelly\\FrontPanelUSB\\APIsetx OPENCV C:\\opencvsetx WX C:\\wxWidgets-3.2.2.1 Open the Start menu and type “edit ENV” into the search bar.Tap “Edit environment variables for your account.”. Add to PATH variable directories: 123456%OPENCV%\\build\\x64\\vc16\\bin%OPENCV%\\build\\x64\\vc15\\bin%OPENCV%\\src\\opencv\\build\\bin%OPENCV%\\src\\opencv\\build\\3rdparty\\ffmpeg%okFP_SDK%\\lib\\x64%WX%\\lib\\vc14x_x64_dll Required DLL’s 1234567891011okFrontPanel.dllopencv_videoio_ffmpeg460_64.dllopencv_videoio_msmf460_64.dllopencv_world460.dllwxbase32u_vc14x_x64.dllwxmsw32u_aui_vc14x_x64.dllwxmsw32u_core_vc14x_x64.dllwxmsw32u_html_vc14x_x64.dllwxmsw32u_propgrid_vc14x_x64.dlllibcurl-x64.dllexiv2.dll Set up the project in Visual Studio 1$(ProjectDir);$(WX)/include/;$(WX)/lib/vc14x_x64_dll/mswud;$(OPENCV)/build/include;$(okFP_SDK)/include;$(ProjectDir)3rd_party;$(ProjectDir)3rd_party/exiv2;$(ProjectDir)3rd_party/thorlabs_pd100;$(ProjectDir)3rd_party/glew/include 1_DEBUG;DEBUG;WXBUILDING;CURL_STATICLIB;%(PreprocessorDefinitions) 1$(OPENCV)/build/x64/vc16/lib;$(OPENCV)/build/x64/vc15/lib;$(OPENCV)/build/lib;$(WX)/lib/vc14x_x64_dll;$(okFP_SDK)/lib/x64;$(ProjectDir)3rd_party/curl;$(ProjectDir)3rd_party/exiv2/Debug;$(ProjectDir)3rd_party/thorlabs_pd100;$(ProjectDir)3rd_party/glew/lib 1wxbase32ud.lib;wxbase32ud_net.lib;wxbase32ud_xml.lib;wxmsw32ud_core.lib;wxmsw32ud_adv.lib;wxmsw32ud_aui.lib;wxmsw32ud_gl.lib;wxmsw32ud_html.lib;wxmsw32ud_media.lib;wxmsw32ud_propgrid.lib;wxmsw32ud_qa.lib;wxmsw32ud_ribbon.lib;wxmsw32ud_richtext.lib;wxmsw32ud_stc.lib;wxmsw32ud_webview.lib;wxmsw32ud_xrc.lib;wxexpatd.lib;wxjpegd.lib;wxpngd.lib;wxregexud.lib;wxscintillad.lib;wxtiffd.lib;wxzlibd.lib;shlwapi.lib;oleacc.lib;uxtheme.lib;$(CoreLibraryDependencies)","link":"/2024/01/04/VS-Property-Configuration-C-C/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy Set up a draft1$ hexo new draft &lt;title&gt; Publish a draft1$ hexo P &lt;filename&gt; Where is the name of the article without the .md suffix. More info: Deployment","link":"/2024/03/31/hello-world/"}],"tags":[{"name":"C++","slug":"C","link":"/tags/C/"},{"name":"FFmpeg","slug":"FFmpeg","link":"/tags/FFmpeg/"},{"name":"OpenCV","slug":"OpenCV","link":"/tags/OpenCV/"},{"name":"OpenGL","slug":"OpenGL","link":"/tags/OpenGL/"},{"name":"Machine Learning","slug":"Machine-Learning","link":"/tags/Machine-Learning/"},{"name":"Grokking-the-ML-system-designs","slug":"Grokking-the-ML-system-designs","link":"/tags/Grokking-the-ML-system-designs/"},{"name":"Deep Learning","slug":"Deep-Learning","link":"/tags/Deep-Learning/"},{"name":"Paper","slug":"Paper","link":"/tags/Paper/"},{"name":"Tools","slug":"Tools","link":"/tags/Tools/"},{"name":"AI Agent","slug":"AI-Agent","link":"/tags/AI-Agent/"},{"name":"LangChain","slug":"LangChain","link":"/tags/LangChain/"}],"categories":[{"name":"Tech","slug":"Tech","link":"/categories/Tech/"},{"name":"Intro","slug":"Tech/Intro","link":"/categories/Tech/Intro/"},{"name":"Grokking-interviews","slug":"Grokking-interviews","link":"/categories/Grokking-interviews/"},{"name":"Hexo","slug":"Hexo","link":"/categories/Hexo/"},{"name":"ML","slug":"Grokking-interviews/ML","link":"/categories/Grokking-interviews/ML/"},{"name":"AI Agent","slug":"AI-Agent","link":"/categories/AI-Agent/"},{"name":"System-Design","slug":"System-Design","link":"/categories/System-Design/"},{"name":"Project","slug":"Project","link":"/categories/Project/"},{"name":"ML","slug":"System-Design/ML","link":"/categories/System-Design/ML/"},{"name":"C++","slug":"Project/C","link":"/categories/Project/C/"}],"pages":[{"title":"Sitemap","text":"Projects Web-based Santorini Game Learning Links @CMU 15-445 Intro to Database @CMU 18-613 Foundations of Computer Systems Friends 小土刀2.0 Lil’Log","link":"/Sitemap/index.html"},{"title":"About me","text":"万言万当，不如一默。","link":"/about/index.html"},{"title":"categories","text":"","link":"/categories/index.html"}]}